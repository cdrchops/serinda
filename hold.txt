I thought I'd chime in on this because there are a couple of items in this thread that interest me (read: i've been working on).

The first item is marker-based tracking.  This is a misnomer, IMO, because all object detection is marker-based.  If you're detecting a chair then that's the marker.  Generally, people talk about marker-based tracking as some form of barcode, qr code, aruco code, hiro, etc.  I point this out because thinking of it in this manner extends the base libraries formed.  Meaning, if you have something that converts camera (and by camera I'm talking about a webcam, usb cam, etc and not the BabylonJS camera for a scene) to coordinates that can be transformed to be used in a BabylonJS scene then you can simply add detection algorithms and pass in the points to the scene and you've got your display.

The second item to note is that most AR or MR is really VR.  You're looking at the display for AR through a screen on a mobile device that is putting the AR pieces and the video from the camera together.  I think for most people this is fine.  They'll be using a mobile device for their viewing.

I have a working version of a webcam tracking a face with OpenCV and then adding a HUD/GUI object to the display.  The area I'm working on now is more accurately translating the OpenCV coordinates to the BabylonJS scene.  I've been reading JSArtoolkit, BabylonAR, and other locations to get this to work.  I'm almost there.

WebXR will not work with a webcam - I didn't see that answer but I thought I'd point that out.  One reply said they didn't think it would - it will not.

My use-case is different from many others because I don't need it to work with a mobile device.  I'm working on integrating a JS version of SLAM into my project.  However, I also will be testing with an Intel RealSense T265 and I have a 435i, I think.  The other part of my use-case that's different is that I'm not using the computer webcam except for initial testing.  My use-case is for an HMD which I've dubbed Johnny Mnemonic because of how incredibly huge the prototype is lol.  I am actually passing my video through to the see-through viewer using OpenCV to create transparent images - so it's really not transparent in that way - it's a darker background

---- second

I'll start with a hopefully brief explanation of my project and then where I'm at.

I've been working on an HMD for a while now.  I use OpenCV to detect data and give me coordinates that I can then send to BabylonJS.  I have several HMD, however, the one I use the most I call Johnny Mnemonic because of how huge the prototype is.  I have my code working with OpenCV via Python and Flask or via OpenCV.js (wasm - through hungxing's examples).  It also detects LeapMotion and supplies some data points (I don't even remember where I found the code (maybe XRinnovations?)) though I've not integrated collision detection, yet.

The HUD/GUI is only one aspect of my work, but this is the portion I'm spending my time on to get it right.  I am not using a mobile device.  Most people, I've found, seem to be using a mobile device for their AR/MR work.  I have external cameras (2 USB cameras and I'm working on integrating the RealSense T265 and 435i just to have more options).  I also have a javascript SLAM library to integrate.  I'm not sure which one at the moment.  I think it's either https://github.com/permutationlock/slam_js or https://github.com/avantgardnerio/slam-client - I started working on ORB-SLAM 2 to compile it for the web, but again, I'm distracted by the HUD/GUI.

The next item, and I'm sorry this is longer than I thought it'd be, is that all object detection is marker-based detection, IMO.  If you're detecting a chair then that's the marker.  Most people think of marker-based detection as hiro, aruco, QR code, barcode, or some variation.