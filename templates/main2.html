<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gesture Recognition with USB Camera</title>
    <style>
        #video, #renderCanvas {
            width: 640px;
            height: 480px;
        }
        #renderCanvas {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
{#    <script src="/static/js/camera_utils.js"></script>#}
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
{#    <script src="/static/js/hands.js"></script>#}
    <script src="https://cdn.babylonjs.com/babylon.js"></script>
{#    <script src="/static/js/babylon.js"></script>#}
    <script src="https://docs.opencv.org/4.5.5/opencv.js"></script>
{#    <script src="/static/js/opencv.js"></script>#}
</head>
<body>
    <video id="video" autoplay></video>
    <canvas id="renderCanvas"></canvas>
    <canvas id="cvCanvas" width="640" height="480" style="display:none;"></canvas>

    <h1><a href="//webrtc.github.io/samples/" title="WebRTC samples homepage">WebRTC samples</a><span>Select sources &amp; outputs</span>
    </h1>

    <p>Get available audio, video sources and audio output devices from <code>mediaDevices.enumerateDevices()</code>
        then set the source for <code>getUserMedia()</code> using a <code>deviceId</code> constraint.</p>
    <p><b>Note:</b> without permission, the browser will restrict the available devices to at most one per type.</p>

    <div class="select">
        <label for="audioSource">Audio input source: </label><select id="audioSource"></select>
    </div>

    <div class="select">
        <label for="audioOutput">Audio output destination: </label><select id="audioOutput"></select>
    </div>

    <div class="select">
        <label for="videoSource">Video source: </label><select id="videoSource"></select>
    </div>

    <div class="select">
        <label for="objectType">Object type: </label>
        <select id="objectType">
            <option value="face" selected>face</option>
            <option value="bottle">bottle</option>
            <option value="phone">phone</option>
        </select>
    </div>

{#    <video id="video" playsinline autoplay></video>#}

    <p><b>Note:</b> If you hear a reverb sound your microphone is picking up the output of your
        speakers/headset, lower the volume and/or move the microphone further away from your speakers/headset.</p>

    <script>
        // Video and canvas setup
        const video = document.getElementById('video');
        const cvCanvas = document.getElementById('cvCanvas');
        const ctx = cvCanvas.getContext('2d');


        // Function to select and start webcam stream
        async function startWebcam() {
            try {
                // Request initial permission to access any camera
                const initialStream = await navigator.mediaDevices.getUserMedia({ video: true });
                // Stop the initial stream to free up the camera
                initialStream.getTracks().forEach(track => track.stop());

                // Enumerate devices to get camera list
                const devices = await navigator.mediaDevices.enumerateDevices();
                const videoDevices = devices.filter(device => device.kind === 'videoinput');

                // Find USB camera (look for "USB" in label, case-insensitive)
                let usbCamera = videoDevices.find(device => device.label.toLowerCase().includes('usb'));
                if (!usbCamera) {
                    console.warn('No USB camera found, falling back to default camera');
                    usbCamera = videoDevices[0]; // Fallback to first available camera
                }

                // Start stream with selected camera
                if (usbCamera) {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        video: { deviceId: { exact: usbCamera.deviceId } }
                    });
                    video.srcObject = stream;
                    console.log('Using camera:', usbCamera.label || 'Unknown');
                } else {
                    console.error('No video devices available');
                }
            } catch (error) {
                console.error('Error accessing webcam:', error);
            }
        }

        // Start the webcam
        startWebcam();

        // BabylonJS setup
        const canvas = document.getElementById('renderCanvas');
        const engine = new BABYLON.Engine(canvas, true);
        const scene = new BABYLON.Scene(engine);
        scene.clearColor = new BABYLON.Color4(0, 0, 0, 0);

        const camera = new BABYLON.FreeCamera('camera', new BABYLON.Vector3(0, 0, -10), scene);
        camera.setTarget(BABYLON.Vector3.Zero());

        const light = new BABYLON.HemisphericLight('light', new BABYLON.Vector3(0, 1, 0), scene);

        // Object manager for multiple detected objects (e.g., faces)
        const objects = new Map(); // id -> { mesh, material, grabOffset, lastAngle }
        let activeObjectId = null; // id of the currently interacted object

        // Object type dropdown handling (declared after 'objects' to avoid TDZ issues)
        const objectSelect = document.getElementById('objectType');
        let selectedObjectType = (objectSelect && objectSelect.value) ? objectSelect.value : 'face';
        if (objectSelect) {
            objectSelect.addEventListener('change', () => {
                selectedObjectType = objectSelect.value;
                // Hide meshes that are not of the selected type and reset active object if it doesn't match
                objects.forEach(({ mesh }, id) => {
                    if (!id.startsWith(`${selectedObjectType}_`)) {
                        mesh.isVisible = false;
                    }
                });
                if (activeObjectId && !activeObjectId.startsWith(`${selectedObjectType}_`)) {
                    activeObjectId = null;
                }
                console.log('Selected object type:', selectedObjectType);
            });
        }

        function getOrCreateObject(id) {
            if (objects.has(id)) return objects.get(id);
            const mesh = BABYLON.MeshBuilder.CreatePlane(id, { size: 1 }, scene);
            const material = new BABYLON.StandardMaterial(`mat_${id}`, scene);
            material.diffuseColor = new BABYLON.Color3(1, 1, 1);
            mesh.material = material;
            mesh.billboardMode = BABYLON.Mesh.BILLBOARDMODE_ALL;
            mesh.isVisible = false;
            const entry = { mesh, material, grabOffset: new BABYLON.Vector3(0, 0, 0), lastAngle: null };
            objects.set(id, entry);
            return entry;
        }

        // Hand wireframe connections (MediaPipe landmarks)
        const connections = [
            [0,1], [1,2], [2,3], [3,4],      // Thumb
            [0,5], [5,6], [6,7], [7,8],      // Index
            [0,9], [9,10], [10,11], [11,12], // Middle
            [0,13], [13,14], [14,15], [15,16], // Ring
            [0,17], [17,18], [18,19], [19,20]  // Pinky
        ];

        // Hand lines
        const handLines = connections.map((_, i) => {
            const line = BABYLON.MeshBuilder.CreateLines(`handLine${i}`, {
                points: [new BABYLON.Vector3(0, 0, 0), new BABYLON.Vector3(0, 0, 0)],
                updatable: true,
                color: new BABYLON.Color3(1, 0, 0)
            }, scene);
            line.isVisible = false;
            return line;
        });

        // Gesture state
        let currentGesture = 'none';
        let isGrabbing = false;
        let wasGrabbing = false;

        // Render loop with gesture-based color change for active object only
        engine.runRenderLoop(() => {
            // Reset all objects to white each frame
            objects.forEach(({ material, mesh }, id) => {
                if (mesh.isVisible) {
                    material.diffuseColor = new BABYLON.Color3(1, 1, 1);
                }
            });
            // Apply gesture color to active object
            if (activeObjectId && objects.has(activeObjectId)) {
                const { material, mesh } = objects.get(activeObjectId);
                if (mesh.isVisible) {
                    switch (currentGesture) {
                        case 'fist':
                            material.diffuseColor = new BABYLON.Color3(1, 0, 0); // Red
                            break;
                        case 'open':
                            material.diffuseColor = new BABYLON.Color3(0, 1, 0); // Green
                            break;
                        case 'two_fingers':
                            material.diffuseColor = new BABYLON.Color3(0, 0, 1); // Blue
                            break;
                        default:
                            material.diffuseColor = new BABYLON.Color3(1, 1, 1); // White
                    }
                }
            }
            scene.render();
        });

        // Coordinate mapping
        const videoWidth = 640;
        const videoHeight = 480;
        const sceneWidth = 6;
        const sceneHeight = 4.5;

        function mapToScene(x, y) {
            const sceneX = (x / videoWidth) * sceneWidth - sceneWidth / 2;
            const sceneY = -((y / videoHeight) * sceneHeight - sceneHeight / 2);
            return [sceneX, sceneY];
        }

        // Distance calculation in 2D (x, y) for simplicity
        function distance(p1, p2) {
            return Math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2);
        }
        function distanceXY(x1, y1, x2, y2) {
            return Math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2);
        }

        function getNearestObject(cursorVec3, maxDist = 1.0) {
            let nearestId = null;
            let nearestDist = Infinity;
            objects.forEach(({ mesh }, id) => {
                if (!mesh.isVisible) return;
                const dx = mesh.position.x - cursorVec3.x;
                const dy = mesh.position.y - cursorVec3.y;
                const d = Math.hypot(dx, dy);
                if (d < nearestDist && d <= maxDist) {
                    nearestDist = d;
                    nearestId = id;
                }
            });
            return nearestId;
        }

        // Gesture detection using all finger landmarks
        function detectGesture(landmarks) {
            const wrist = landmarks[0];
            const fingers = [
                { base: 2, tip: 4 }, // Thumb (use IP joint as base)
                { base: 5, tip: 8 }, // Index
                { base: 9, tip: 12 }, // Middle
                { base: 13, tip: 16 }, // Ring
                { base: 17, tip: 20 } // Pinky
            ];

            // Determine finger state (extended or curled)
            const states = fingers.map(f => {
                const base = landmarks[f.base];
                const tip = landmarks[f.tip];
                const dist_base_wrist = distance(wrist, base);
                const dist_tip_wrist = distance(wrist, tip);
                return dist_tip_wrist > dist_base_wrist * 1.2 ? 'extended' : 'curled';
            });

            // Count extended fingers
            const extendedCount = states.filter(s => s === 'extended').length;

            // Define gestures
            if (extendedCount === 0) {
                return 'fist'; // All fingers curled
            } else if (extendedCount >= 4) {
                return 'open'; // Most or all fingers extended
            } else if (states[1] === 'extended' && states[2] === 'extended' && states[0] === 'curled' && states[3] === 'curled' && states[4] === 'curled') {
                return 'two_fingers'; // Index and middle extended, others curled
            } else {
                return 'none';
            }
        }

        // MediaPipe Hands setup
        const hands = new Hands({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}` });
        hands.setOptions({
            maxNumHands: 1,
            modelComplexity: 1,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });
        hands.onResults(onResults);

        const cameraFeed = new Camera(video, {
            onFrame: async () => await hands.send({ image: video }),
            width: 640,
            height: 480
        });
        cameraFeed.start();

        // Hand tracking results + gesture interactions (pinch to grab, move, rotate)
        function onResults(results) {
            if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
                const landmarks = results.multiHandLandmarks[0];
                currentGesture = detectGesture(landmarks);

                // Update hand lines
                connections.forEach((conn, i) => {
                    const p1 = landmarks[conn[0]];
                    const p2 = landmarks[conn[1]];
                    const [x1, y1] = mapToScene(p1.x * videoWidth, p1.y * videoHeight);
                    const [x2, y2] = mapToScene(p2.x * videoWidth, p2.y * videoHeight);
                    const points = [
                        new BABYLON.Vector3(x1, y1, 0),
                        new BABYLON.Vector3(x2, y2, 0)
                    ];
                    BABYLON.MeshBuilder.CreateLines(`handLine${i}`, {
                        points,
                        updatable: true,
                        instance: handLines[i]
                    }, scene);
                    handLines[i].isVisible = true;
                });

                // Compute "pinch" between thumb tip (4) and index tip (8)
                const thumbTip = landmarks[4];
                const indexTip = landmarks[8];
                const pinchDistanceNorm = distance(thumbTip, indexTip); // normalized space (0..1)
                // Dynamic threshold scaled to video pixels for stability
                const pinchPx = distanceXY(thumbTip.x * videoWidth, thumbTip.y * videoHeight, indexTip.x * videoWidth, indexTip.y * videoHeight);
                const pinchThresholdPx = 35; // tune between 25-45 if needed

                wasGrabbing = isGrabbing;
                isGrabbing = pinchPx < pinchThresholdPx;

                // Map index tip to scene coordinates for cursor/dragging
                const [cursorX, cursorY] = mapToScene(indexTip.x * videoWidth, indexTip.y * videoHeight);
                const cursor = new BABYLON.Vector3(cursorX, cursorY, 0);

                if (isGrabbing && !wasGrabbing) {
                    // On grab start: choose nearest visible object to the cursor
                    const candidateId = getNearestObject(cursor, 1.5);
                    if (candidateId) {
                        activeObjectId = candidateId;
                        const entry = objects.get(activeObjectId);
                        // Compute offset from cursor to object to avoid snapping
                        entry.grabOffset = entry.mesh.position.subtract(cursor);
                        // Initialize rotation tracking based on index finger direction (5 -> 8)
                        const indexMCP = landmarks[5];
                        const vX = (indexTip.x - indexMCP.x);
                        const vY = (indexTip.y - indexMCP.y);
                        entry.lastAngle = Math.atan2(vY, vX);
                        // Ensure the object is visible when grabbing
                        entry.mesh.isVisible = true;
                    } else {
                        activeObjectId = null;
                    }
                }

                if (isGrabbing && activeObjectId && objects.has(activeObjectId)) {
                    const entry = objects.get(activeObjectId);
                    // Move: follow cursor with offset
                    entry.mesh.position = cursor.add(entry.grabOffset);
                    // Rotate: compute angle delta from index finger orientation
                    const indexMCP = landmarks[5];
                    const vX = (indexTip.x - indexMCP.x);
                    const vY = (indexTip.y - indexMCP.y);
                    const angle = Math.atan2(vY, vX);
                    if (entry.lastAngle !== null) {
                        const delta = angle - entry.lastAngle;
                        entry.mesh.rotation.z += delta; // rotate around z axis
                    }
                    entry.lastAngle = angle;
                } else {
                    // On release: reset rotation tracker
                    if (wasGrabbing && activeObjectId && objects.has(activeObjectId)) {
                        const entry = objects.get(activeObjectId);
                        entry.lastAngle = null;
                    }
                    if (!isGrabbing) {
                        activeObjectId = null;
                    }
                }
            } else {
                currentGesture = 'none';
                handLines.forEach(line => line.isVisible = false);
                wasGrabbing = isGrabbing;
                isGrabbing = false;
                activeObjectId = null;
            }
        }

        // OpenCV.js face detection
        let classifier;
        cv['onRuntimeInitialized'] = async () => {
            const response = await fetch('https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml');
            {#const response = await fetch('/static/opencv/haarcascade_frontalface_default.xml');#}
            const buffer = await response.arrayBuffer();
            cv.FS_createDataFile('/', 'cascade.xml', new Uint8Array(buffer), true, false, false);
            classifier = new cv.CascadeClassifier();
            classifier.load('cascade.xml');
            detectFaces();
        };

        function detectFaces() {
            ctx.drawImage(video, 0, 0, 640, 480);
            const imageData = ctx.getImageData(0, 0, 640, 480);
            const src = cv.matFromImageData(imageData);
            cv.cvtColor(src, src, cv.COLOR_RGBA2GRAY);
            const faces = new cv.RectVector();
            classifier.detectMultiScale(src, faces);

            {#cv.flip(src, 1);#}

            // This calculation TODO: will determine object distance and then create a calculated amount of pixels to
            //    move the box up to the top of the item
            const calculation =  - 250;


            if (faces.size() > 0) {
                const seenIds = new Set();
                for (let i = 0; i < faces.size(); i++) {
                    const face = faces.get(i);
                    const centerX = face.x + face.width / 2;
                    const centerY = face.y - face.height;
                    const [sceneX, sceneY] = mapToScene(centerX, centerY);
                    const id = `${selectedObjectType}_${i}`;
                    const entry = getOrCreateObject(id);
                    // Only update from face detection when not actively grabbing this object
                    if (!(isGrabbing && activeObjectId === id)) {
                        entry.mesh.position.set(sceneX, sceneY, 0);
                    }
                    entry.mesh.isVisible = true;
                    seenIds.add(id);
                }
                // Hide face objects not seen in this frame (unless being grabbed)
                objects.forEach(({ mesh }, id) => {
                    if (id.startsWith(`${selectedObjectType}_`) && !seenIds.has(id)) {
                        if (!(isGrabbing && activeObjectId === id)) {
                            mesh.isVisible = false;
                        }
                    }
                });
            } else {
                // No faces: hide all objects of the selected type unless being grabbed
                objects.forEach(({ mesh }, id) => {
                    if (id.startsWith(`${selectedObjectType}_`)) {
                        if (!(isGrabbing && activeObjectId === id)) {
                            mesh.isVisible = false;
                        }
                    }
                });
            }

            src.delete();
            faces.delete();
            requestAnimationFrame(detectFaces);
        }
    </script>
    <script src="https://webrtc.github.io/adapter/adapter-latest.js"></script>
{#    <script src="/static/js/adapter-latest.js"></script>#}
    <script src="/static/main.js" async></script>
</body>
</html>